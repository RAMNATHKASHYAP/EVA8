# Import necessary modules from PyTorch library
from __future__ import print_function
import torch
import torch.nn as nn # module for building neural networks
import torch.nn.functional as F # functional interface for building neural networks
import torch.optim as optim # module for optimization algorithms
from torchvision import datasets, transforms # modules for loading and preprocessing data


class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        # Define 2D convolutional layer with 1 input channel, 16 output channels and kernel size of 3
        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)
        # Define batch normalization layer for output of conv1
        self.bn1 = nn.BatchNorm2d(16)
        # Define 2D convolutional layer with 16 input channels, 32 output channels and kernel size of 3
        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)
        # Define batch normalization layer for output of conv2
        self.bn2 = nn.BatchNorm2d(32)
        # Define max pooling layer with kernel size of 2 and stride of 2
        self.pool = nn.MaxPool2d(2, 2)
        # Define 2D convolutional layer with 32 input channels, 64 output channels and kernel size of 3
        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)
        # Define batch normalization layer for output of conv3
        self.bn3 = nn.BatchNorm2d(64)
        # Define 2D convolutional layer with 64 input channels, 128 output channels and kernel size of 3
        self.conv4 = nn.Conv2d(64, 128, 3, padding=1)
        # Define batch normalization layer for output of conv4
        self.bn4 = nn.BatchNorm2d(128)
        # Define 2D convolutional layer with 128 input channels and 256 output channels, and kernel size of 3
        self.conv5 = nn.Conv2d(128, 256, 3)
        # Define batch normalization layer for output of conv5
        self.bn5 = nn.BatchNorm2d(256)
        # Define AdaptiveAvgPool2d layer to aggregate global spatial information and reduce the spatial dimensions
        self.gap = nn.AdaptiveAvgPool2d(1)
        # Define fully connected linear layer with 256 input channels and 10 output channels
        self.fc = nn.Linear(256, 10)
        # Define dropout layer with drop probability of 0.25
        self.dropout = nn.Dropout(0.25)

    def forward(self, x):
        # Apply conv1, bn1, relu, pool, conv2, bn2, relu, pool, conv3, bn3, relu, conv4, bn4, relu, conv5, bn5, relu, adaptive avg pool and dropout layers in sequence to input tensor x
        x = self.pool(F.relu(self.bn2(self.conv2(F.relu(self.bn1(self.conv1(x)))))))
        x = self.pool(F.relu(self.bn4(self.conv4(F.relu(self.bn3(self.conv3(x)))))))
        x = self.gap(F.relu(self.bn5(self.conv5(x))))
        # Reshape tensor to have shape of (-1, 256)
        x = x.view(-1, 256)
        # Apply dropout
        x = self.dropout(x)
        # Apply fully connected linear layer
        x = self.fc(x)
        # Apply log_softmax function to output along second dimension (dim=1)
        return F.log_softmax(x, dim=1)


# Install torchsummary package
!pip install torchsummary

# Import summary function from torchsummary
from torchsummary import summary

# Check if CUDA-enabled GPU is available
use_cuda = torch.cuda.is_available()

# Set device to "cuda" if GPU is available, otherwise set to "cpu"
device = torch.device("cuda" if use_cuda else "cpu")

# Create an instance of the Net class and move to specified device
model = Net().to(device)

# Display a summary of the model's architecture, including number of trainable parameters and output size of each layer
summary(model, input_size=(1, 28, 28))

...........................................................................................................
      Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/
Requirement already satisfied: torchsummary in /usr/local/lib/python3.8/dist-packages (1.5.1)
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 16, 28, 28]             160
       BatchNorm2d-2           [-1, 16, 28, 28]              32
            Conv2d-3           [-1, 32, 28, 28]           4,640
       BatchNorm2d-4           [-1, 32, 28, 28]              64
         MaxPool2d-5           [-1, 32, 14, 14]               0
            Conv2d-6           [-1, 64, 14, 14]          18,496
       BatchNorm2d-7           [-1, 64, 14, 14]             128
            Conv2d-8          [-1, 128, 14, 14]          73,856
       BatchNorm2d-9          [-1, 128, 14, 14]             256
        MaxPool2d-10            [-1, 128, 7, 7]               0
           Conv2d-11            [-1, 256, 5, 5]         295,168
      BatchNorm2d-12            [-1, 256, 5, 5]             512
AdaptiveAvgPool2d-13            [-1, 256, 1, 1]               0
          Dropout-14                  [-1, 256]               0
           Linear-15                   [-1, 10]           2,570
================================================================
Total params: 395,882
Trainable params: 395,882
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.00
Forward/backward pass size (MB): 1.35
Params size (MB): 1.51
Estimated Total Size (MB): 2.86
....................................................................................................................


# Set random seed for reproducibility
torch.manual_seed(1)

# Set batch size for dataloaders
batch_size = 128

# Create kwargs dictionary for DataLoader
if use_cuda:
    kwargs = {'num_workers': 1, 'pin_memory': True}
else:
    kwargs = {}

# Create train_loader using DataLoader class
# MNIST training dataset is downloaded if not present and transformed using ToTensor() and Normalize()
train_loader = torch.utils.data.DataLoader(
    datasets.MNIST('../data', train=True, download=True,
                    transform=transforms.Compose([
                        transforms.ToTensor(),
                        transforms.Normalize((0.1307,), (0.3081,))
                    ])),
    batch_size=batch_size, shuffle=True, **kwargs)

# Create test_loader using DataLoader class
# MNIST testing dataset is transformed using ToTensor() and Normalize()
test_loader = torch.utils.data.DataLoader(
    datasets.MNIST('../data', train=False, transform=transforms.Compose([
                        transforms.ToTensor(),
                        transforms.Normalize((0.1307,), (0.3081,))
                    ])),
    batch_size=batch_size, shuffle=True, **kwargs)
 ..........................................................................................................................   
            Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz
Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz
  0%|          | 0/9912422 [00:00<?, ?it/s]
Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw

Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz
Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz
  0%|          | 0/28881 [00:00<?, ?it/s]
Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw

Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz
Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz
  0%|          | 0/1648877 [00:00<?, ?it/s]
Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw

Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz
Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz
  0%|          | 0/4542 [00:00<?, ?it/s]
Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw

........................................................................................................................
from tqdm import tqdm

def train(model, device, train_loader, optimizer, epoch):
    # set the model to train mode
    model.train()
    # initialize a progress bar
    pbar = tqdm(train_loader)
    # iterate over the training data
    for batch_idx, (data, target) in enumerate(pbar):
        # move data and targets to the device
        data, target = data.to(device), target.to(device)
        # set gradients of the optimizer to zero
        optimizer.zero_grad()
        # perform a forward pass of the data through the model
        output = model(data)
        # calculate the negative log-likelihood loss
        loss = F.nll_loss(output, target)
        # perform backpropagation to compute gradients
        loss.backward()
        # update the model's parameters
        optimizer.step()
        # update the progress bar with the current loss and batch number
        pbar.set_description(desc= f'loss={loss.item()} batch_id={batch_idx}')

def test(model, device, test_loader):
    # set the model to evaluation mode
    model.eval()
    # initialize a test loss variable
    test_loss = 0
    # initialize a variable to keep track of the number of correct predictions
    correct = 0
    # iterate over the test data with torch.no_grad() to reduce memory usage
    with torch.no_grad():
        for data, target in test_loader:
            # move data and targets to the device
            data, target = data.to(device), target.to(device)
            # perform a forward pass of the data through the model
            output = model(data)
            # accumulate the negative log-likelihood loss
            test_loss += F.nll_loss(output, target, reduction='sum').item()
            # find the index of the maximum log-probability for each sample
            pred = output.argmax(dim=1, keepdim=True)
            # compare the prediction to the true target and increment the number of correct predictions if they match
            correct += pred.eq(target.view_as(pred)).sum().item()
    # calculate average test loss
    test_loss /= len(test_loader.dataset)
    # print the average test loss and the accuracy of the model on the test set
    print('\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(
        test_loss, correct, len(test_loader.dataset),
        100. * correct / len(test_loader.dataset)))


# Create instance of the Net class
model = Net().to(device)
# Create instance of the Adam optimizer
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Train model for 20 epochs
for epoch in range(20):
    # Call train function
    train(model, device, train_loader, optimizer, epoch)
    # Call test function
    test(model, device, test_loader)
    
    
 ...................................................................................................   
    loss=0.04908733069896698 batch_id=468: 100%|██████████| 469/469 [00:17<00:00, 26.56it/s]

Test set: Average loss: 0.0589, Accuracy: 9821/10000 (98%)

loss=0.04376786947250366 batch_id=468: 100%|██████████| 469/469 [00:16<00:00, 28.49it/s]

Test set: Average loss: 0.0413, Accuracy: 9881/10000 (99%)

loss=0.03448459878563881 batch_id=468: 100%|██████████| 469/469 [00:16<00:00, 28.63it/s]

Test set: Average loss: 0.0366, Accuracy: 9885/10000 (99%)

loss=0.004067536443471909 batch_id=468: 100%|██████████| 469/469 [00:15<00:00, 29.33it/s]

Test set: Average loss: 0.0374, Accuracy: 9881/10000 (99%)

loss=0.011334004811942577 batch_id=468: 100%|██████████| 469/469 [00:16<00:00, 28.37it/s]

Test set: Average loss: 0.0346, Accuracy: 9889/10000 (99%)

loss=0.005490738432854414 batch_id=468: 100%|██████████| 469/469 [00:16<00:00, 28.92it/s]

Test set: Average loss: 0.0297, Accuracy: 9905/10000 (99%)

loss=0.020234130322933197 batch_id=468: 100%|██████████| 469/469 [00:16<00:00, 29.14it/s]

Test set: Average loss: 0.0220, Accuracy: 9925/10000 (99%)

loss=0.029146580025553703 batch_id=468: 100%|██████████| 469/469 [00:16<00:00, 28.83it/s]

Test set: Average loss: 0.0309, Accuracy: 9901/10000 (99%)

loss=0.018106330186128616 batch_id=468: 100%|██████████| 469/469 [00:16<00:00, 28.24it/s]

Test set: Average loss: 0.0306, Accuracy: 9902/10000 (99%)

loss=0.005745166912674904 batch_id=468: 100%|██████████| 469/469 [00:15<00:00, 29.47it/s]

Test set: Average loss: 0.0271, Accuracy: 9906/10000 (99%)

loss=0.009812834672629833 batch_id=468: 100%|██████████| 469/469 [00:16<00:00, 29.20it/s]

Test set: Average loss: 0.0216, Accuracy: 9928/10000 (99%)

loss=0.004514332860708237 batch_id=468: 100%|██████████| 469/469 [00:15<00:00, 29.44it/s]

Test set: Average loss: 0.0284, Accuracy: 9909/10000 (99%)

loss=0.0014399358769878745 batch_id=468: 100%|██████████| 469/469 [00:16<00:00, 29.04it/s]

Test set: Average loss: 0.0195, Accuracy: 9936/10000 (99%)

loss=0.0008799185161478817 batch_id=468: 100%|██████████| 469/469 [00:15<00:00, 29.36it/s]

Test set: Average loss: 0.0258, Accuracy: 9921/10000 (99%)

loss=0.008944611065089703 batch_id=468: 100%|██████████| 469/469 [00:15<00:00, 29.69it/s]

Test set: Average loss: 0.0251, Accuracy: 9924/10000 (99%)

loss=0.0003064881020691246 batch_id=468: 100%|██████████| 469/469 [00:16<00:00, 28.93it/s]

Test set: Average loss: 0.0265, Accuracy: 9920/10000 (99%)

loss=0.015349198132753372 batch_id=468: 100%|██████████| 469/469 [00:16<00:00, 28.48it/s]

Test set: Average loss: 0.0224, Accuracy: 9937/10000 (99%)

loss=0.0010630810866132379 batch_id=468: 100%|██████████| 469/469 [00:18<00:00, 25.82it/s]

Test set: Average loss: 0.0207, Accuracy: 9932/10000 (99%)

loss=0.0008517908863723278 batch_id=468: 100%|██████████| 469/469 [00:16<00:00, 28.11it/s]

Test set: Average loss: 0.0259, Accuracy: 9923/10000 (99%)

loss=0.03284295275807381 batch_id=468: 100%|██████████| 469/469 [00:16<00:00, 28.73it/s]

Test set: Average loss: 0.0239, Accuracy: 9925/10000 (99%)

​
